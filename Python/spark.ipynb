{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2c2c4d99-e732-485c-84f3-cf17251a3081",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### PySpark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ade762e4-1098-4c56-b5c6-d6a3352af5f7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\"PySpark is the Python API for Apache Spark, used to process massive datasets (TB/PB) across distributed clusters. It‚Äôs faster than Pandas for big data, supports SQL/ML/streaming, and scales horizontally. We use it when data outgrows single-machine tools.\"\n",
    "###  Bonus Keywords:\n",
    "- RDD (Resilient Distributed Dataset): PySpark‚Äôs core data structure.\n",
    "- DataFrame API: SQL-like operations (similar to Pandas).\n",
    "- Spark SQL: Run SQL queries directly on big data.\n",
    "\n",
    "### Rule of Thumb:\n",
    "- Use Pandas for small data (<1GB, quick analysis).\n",
    "- Use PySpark for big data (or when you need scaling)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "92212194-7c6e-487b-a5c9-057f7d196285",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"Spark DataFrames\").getOrCreate()\n",
    "print (\"Spark session created: \", spark.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3d9f4860-1072-4ab3-aa02-b47d0c95811c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "Understand What a DataFrame Is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2bf1ed37-c74a-473c-a989-7928337ed9c9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "#Create Spark Dataframe \n",
    "\n",
    "data=[('Gourav',50000),('Priya',67000),('Ravi',45000),('SDC',90000),('Ankur',40000)]\n",
    "\n",
    "column=['Name','Salary']\n",
    "\n",
    "spark_df=spark.createDataFrame(data, column)\n",
    "display(spark_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "588b4787-6af5-46f1-86cd-d60a0272610e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Pandas Dataframe\n",
    "import pandas as pd \n",
    "data=[('Gourav',50000),('Priya',67000),('Ravi',45000),('SDC',90000),('Ankur',40000)]\n",
    "\n",
    "column=['Name','Salary']\n",
    "\n",
    "pd_df=pd.DataFrame(data, columns=column)\n",
    "display(pd_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8be9fe65-c13d-4412-9b3e-1d4c2f6ef304",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3d513f84-794c-43be-a434-6e1967844cda",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"Counts in the DF\",df.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6666b952-545d-464d-a926-bc141ba17abf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Real-Life Use Case\n",
    "These DataFrames are used for:\n",
    "- Ingesting data from files\n",
    "- Transforming at scale\n",
    "- Writing to Delta Tables or Parquet files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "48322cb3-35af-48f7-851e-b5d45628665e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "#Add Data Types \n",
    "from pyspark.sql.types import StructType,StructField, StringType, IntegerType\n",
    "schema = StructType([\n",
    "    StructField(\"Name\", StringType(), True),\n",
    "    StructField(\"Salary\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "df_typed = spark.createDataFrame(data, schema=schema)\n",
    "df_typed.show()\n",
    "df_typed.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "80d89c76-88ce-4df8-81d2-8e9e18857053",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Q: What's the difference between SparkSession and SparkContext?**\n",
    "\n",
    "‚úÖ Answer:\n",
    "\n",
    "- SparkSession is the unified entry point from Spark 2.0 onwards.\n",
    "- It internally manages SparkContext, SQLContext, and HiveContext.\n",
    "\n",
    "**SparkContext is the old entry point for RDD operations, while SparkSession (introduced in Spark 2.0) unifies access to DataFrames, SQL, and streaming. Today, we mostly use SparkSession, which internally manages SparkContext for backward compatibility.\"**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "106032d2-ca62-454a-8f86-7b4c055bffcb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "json_data = [\n",
    "    {\"Name\": \"David\", \"Salary\": 70000},\n",
    "    {\"Name\": \"Eva\", \"Salary\": 65000}\n",
    "]\n",
    "\n",
    "df_json=spark.createDataFrame(json_data)\n",
    "display(df_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a10e1850-0e2e-47dd-a5fe-a61198badde0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1550647f-3f4c-4d09-ae7f-e1c27a2e4f8c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"DataX_Bootcamp\").getOrCreate()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "14bf060e-2176-4677-96b6-5d2dbabea6cd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "#create Realistic Dataframe of Employee\n",
    "\n",
    "data={\n",
    "        (\"Alice\",\"IT\",50000),\n",
    "        (\"Gourav\",\"Data Engineer\",100000),\n",
    "        (\"Bob\",\"HR\",40000),\n",
    "        (\"SDC\",\"Recruiter\",150000),\n",
    "        (\"Ayushi\",\"HR\",30000),\n",
    "        (\"Ankur\",\"IT\",510000),\n",
    "        (\"Shruti\",\"Finance\",650000),\n",
    "        (\"Shalini\",\"Finance\",850000),\n",
    "}\n",
    "column=[\"Name\",\"Department\",\"Salary\"]\n",
    "df= spark.createDataFrame(data,column)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "60d5aa60-11f0-4297-9de3-d5d81c7d4086",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Select and filter on the Dataframe Created\n",
    "display(df.select(\"Name\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "651d5687-f305-4c2c-b490-643236a6c7d8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    " display(df.select(\"Name\",\"Salary\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cb780767-2bfe-4f2e-b35b-24352ad91de2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#filter rows based on salary\n",
    "display(df.filter(df.Salary>50000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eee3c53e-a6d5-4a96-ae92-3d7893d1367e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#filter rows based on salary\n",
    "\n",
    "display(df.filter(df.Salary>100000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e529d55d-1097-4b5a-8aa4-16a738fbf1a1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Apply multiple filters\n",
    "display(df.filter((df[\"Department\"] == \"Finance\") & (df[\"Salary\"] > 60000)))\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "26f86a1a-d456-47ee-9639-501de4165a36",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Create a Derived Column\n",
    "\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "#Add 10% Bonus to everyone \n",
    "df=df.withColumn(\"Bonus\",col(\"Salary\")*0.1)\n",
    "display(df)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "17924496-1bcb-4a9b-8cfc-281c1759e193",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Group by and Aggregation\n",
    "#average of salary by department \n",
    "\n",
    "from pyspark.sql.functions import avg\n",
    "df.groupBy(\"Department\").avg(\"Salary\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8f875fbb-00bf-409a-b0f5-51343182b6c7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Group by and Aggregation\n",
    "#average of salary by department\n",
    "from pyspark.sql.functions import avg, min, max\n",
    "display(df.groupBy(\"Department\").agg(\n",
    "    avg(\"Salary\").alias(\"avg_salary\"),\n",
    "    min(\"salary\").alias(\"min_salary\"),\n",
    "    max(\"salary\").alias(\"max_salary\")\n",
    "    )) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3d95e28b-4fe0-4113-8638-f13b449a6eaa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# join - combine tw0 dataframes\n",
    "# create dept code mapping\n",
    "dept_data = [(\"IT\", 101),(\"HR\", 202),(\"Finance\",303)]\n",
    "dep_columns = [\"Department\", \"Dept_Code\"]\n",
    "dept_df = spark.createDataFrame(data=dept_data, schema=dep_columns)\n",
    "\n",
    "# join operations\n",
    "joined_df  = df.join(dept_df, on=\"Department\",how=\"inner\")\n",
    "display(joined_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6d961fab-4603-46c0-b1dc-e813214182e2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Transformations V/s Action\n",
    "#Transformations\n",
    "#Transformations are lazy operations. They are not executed immediately.\n",
    "#They are executed only when an action is called.\n",
    "#Transformations are used to create a new DataFrame from an existing DataFrame.\n",
    "filtered = df.filter(df.Salary>50000)\n",
    "\n",
    "# Action  \n",
    "display(filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "49cfebb4-df64-4695-b1d9-2e69811470dd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Student Practice Assignment\n",
    "## \n",
    "üìÇ Build a DataFrame with the following schema: Name, Department, Salary, Location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0fe31946-35ac-4ea3-bcee-c0fd335ded38",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# sample data\n",
    "data =[\n",
    "     (\"Gourav\", \"IT\", 70000, \"Bangalore\"),\n",
    "    (\"John\", \"HR\", 55000, \"Mumbai\"),\n",
    "    (\"Ravi\", \"Finance\", 65000, \"Delhi\"),\n",
    "    (\"Sneha\", \"IT\", 60000, \"Hyderabad\")\n",
    "]\n",
    "columns = [\"Name\", \"Department\", \"Salary\", \"Location\"]\n",
    "df_task = spark.createDataFrame(data,columns)\n",
    "\n",
    "# Task 1: Filter IT employees with salary > 60K\n",
    "df_task.filter((col(\"Department\")==\"IT\")& (col(\"Salary\")>60000)).show()\n",
    "# Task 2: Add column \"Hike_Amount\" as 15% of salary\n",
    "df_task = df_task.withColumn(\"Hike_Amount\",col(\"Salary\")*0.15)\n",
    "df_task.show()\n",
    "# Task 3: Group by Department and show average salary\n",
    "df_task.groupBy(\"Department\").avg(\"Salary\").show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ab56f9e5-1d0f-48a6-aa2a-c44b7a8a24b5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"DeltaVersioningDemo\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "50134812-a152-4d88-ae43-5c119c4ae974",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data = [(\"Gourav\", \"IT\", 50000), (\"SDC\", \"HR\", 45000), (\"Vaishnav\", \"IT\", 60000)]\n",
    "cols = [\"Name\", \"Department\", \"Salary\"]\n",
    "df = spark.createDataFrame(data,cols)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7d8ea62b-45c0-4371-935b-c4b039e18b27",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"employee_delta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "564b7139-b146-4d12-80ab-1d91e334def7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "update employee_delta set Salary= 10000000 where name ='SDC'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ef8b4672-65cf-45e3-91ae-b27b5adf068f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "describe history employee_delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aee972c7-aaf0-4ec2-8ff2-1cb01e5b9460",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# MERGE INTO ‚ûù Update a Row\n",
    "from pyspark.sql import Row \n",
    "update_data = [Row (name = \"Alice\", Department = \"IT\", Salary = 60000)]\n",
    "df_update = spark.createDataFrame(update_data)\n",
    "df_update.createOrReplaceGlobalTempView(\"updates_view\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "MERGE INTO employee_delta AS target\n",
    "USING updates_view AS source\n",
    "ON target.Name = source.Name\n",
    "WHEN MATCHED THEN UPDATE SET *\n",
    "WHEN NOT MATCHED THEN INSERT *\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "692dc801-d047-464e-a597-90e67dca37df",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "describe history employee_delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "949b63e1-4842-4aa6-b8dd-26c48ce1bd18",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# schema Evolution (Add a New Coloumn)\n",
    "new_data = [(\"David\", \"Finance\", 70000, 5)] # 5 yrs experience\n",
    "cols_new = [\"Name\", \"Department\", \"Salary\", \"Experience\"]\n",
    "df_new = spark.createDataFrame(new_data,cols_new)\n",
    "display(df_new)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "61db089b-ca43-49fd-a823-28d9ce89df7c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql \n",
    "\n",
    "describe extended  employee_delta\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8439013285628018,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "spark",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
