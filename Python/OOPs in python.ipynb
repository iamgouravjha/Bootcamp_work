{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d6fc2d0a-7b59-4c1c-9d77-6d31721f0382",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# OOPs in python\n",
    "- Data Engineers build resusable tools: readers, transformers, loggers.\n",
    "- OOP help us modularize, scale, and reorganize pipeline steps cleanly.\n",
    "- Instead of repeating code for each file format/API - Define a class once use everywhere."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a33065e6-ee52-4382-a3ef-0dae73d375af",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#classes and objects (Core Foundataion)\n",
    "###What are Classes & Objects?\n",
    "- Class: A blueprint for creating objects (defines attributes & methods).\n",
    "- Object: An instance of a class (contains real data & behavior).\n",
    "###Key Concepts:\n",
    "- __init__: Constructor (initializes object attributes).\n",
    "\n",
    "- Methods: Functions defined inside a class (actions an object can perform).\n",
    "\n",
    "- Instance Variables: Data unique to each object.\n",
    "\n",
    "###Why Important in Data Engineering?\n",
    "- Modularity: Breaks pipelines into reusable components (e.g., Extract, Transform, Load).\n",
    "\n",
    "- Abstraction: Hides complex logic behind simple method calls (e.g., execute()).\n",
    "\n",
    "- Scalability: Easily add new steps without rewriting existing code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9a456770-90a3-4549-b182-e970742b0ef3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class customer:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "c1 = customer(\"Gourav\")\n",
    "print(c1.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "57574e75-f4f1-4a0f-b246-6ac1d540ff6a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class PipelineStep:\n",
    "    def __init__(self, step_name):  # Constructor\n",
    "        self.step_name = step_name  # Instance variable (unique per object)\n",
    "\n",
    "    def execute(self):  # Method (action)\n",
    "        print(\"Executing step: \" + self.step_name)\n",
    "\n",
    "# Objects (instances of PipelineStep)\n",
    "ingest = PipelineStep(\"Ingestion\")  \n",
    "transform = PipelineStep(\"Transformation\")  \n",
    "\n",
    "ingest.execute()    # Output: \"Executing step: Ingestion\"\n",
    "transform.execute() # Output: \"Executing step: Transformation\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "46bf0c21-0315-4046-b29c-447c7851a2ca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Encapsulation -protect Internal Logic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8a1d723f-5868-44dd-8c36-882631c4018b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##What is Encapsulation?\n",
    "- Bundling data (attributes) and methods (functions) into a single unit (class).\n",
    "- Restricting direct access to sensitive data (data hiding).\n",
    "- Provides security, maintainability, and controlled access.\n",
    "###Why is it Important in Data Engineering?\n",
    "- Protects sensitive data (e.g., database credentials, API keys).\n",
    "- Hides complex logic (e.g., ETL transformations, connection handling).\n",
    "- Improves modularity‚Äîchanges inside a class don‚Äôt affect other code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2f8f590d-1405-46fc-91d4-889875fc546e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class databaseconnector:\n",
    "    def __init__(self):\n",
    "        self.credentials = \"user:pass@123\"  #Protected variable credentials, The underscore _ prefix indicates this is a protected variable\n",
    "    def connect (self):\n",
    "            print(\"Connecting to database using credentials: \")\n",
    "            return \"DB connection established\"\n",
    "db = databaseconnector()\n",
    "# print(db.credentials)\n",
    "print(db.connect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "63a719b9-6475-4cd9-a496-87b6cfae73be",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Inheritance in Python\n",
    "### What is Inheritance?\n",
    "- Reuse & Extend: A child class inherits attributes/methods from a parent class.\n",
    "- Method Overriding: Child classes can modify inherited methods.\n",
    "- Hierarchy: Creates logical relationships (e.g., BaseReader ‚Üí CSVReader, APIReader).\n",
    "###Why Important in Data Engineering?\n",
    "- Avoids Code Duplication: Shared logic (e.g., read()) in a base class.\n",
    "- Standardizes Interfaces: All readers must implement read().\n",
    "- Extensibility: Add new readers (e.g., DBReader) without changing existing code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ce6c1b07-1d2b-4408-afdf-4862c4fe6af8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Example: Reader Class Hierarchy\n",
    "class Reader():  # Parent class (Base)\n",
    "    def read(self):\n",
    "        return \"Reading from Base Reader...\"  # Default implementation\n",
    "\n",
    "class CSVReader(Reader):  # Child class (inherits from Reader)\n",
    "    def read(self):  # Method overriding\n",
    "        return \"üìÑ Reading from CSV\"\n",
    "\n",
    "class APIReader(Reader):  # Child class\n",
    "    def read(self):  # Method overriding\n",
    "        return \"üåê Fetching from API\"\n",
    "\n",
    "# Usage\n",
    "print(CSVReader().read())  # Output: \"üìÑ Reading from CSV\"  \n",
    "print(APIReader().read())  # Output: \"üåê Fetching from API\"  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "17eb0908-ceb6-4a09-beb9-fed892b83665",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Real-World Use Case:\n",
    "class DataSource:  # Base class\n",
    "    def extract(self):\n",
    "        raise NotImplementedError(\"Child classes must implement this!\")\n",
    "\n",
    "class BigQuerySource(DataSource):\n",
    "    def extract(self):\n",
    "        return \"Extracting from BigQuery...\"\n",
    "\n",
    "class S3Source(DataSource):\n",
    "    def extract(self):\n",
    "        return \"Loading from S3...\"\n",
    "    \n",
    "source = BigQuerySource()\n",
    "print(source.extract())     # Output:Extracting from BigQuery...\n",
    "load = S3Source()\n",
    "print(load.extract())       # Output: Loading from S3..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8234fe47-89b0-4c58-893a-b80191548094",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Polymorphism ‚Äì Same Interface, Different Behavior\n",
    "### What is Polymorphism?\n",
    "- \"Many Forms\": One interface (e.g., method name) with different implementations.\n",
    "- Shared Behavior: Objects of different classes respond to the same method call (read()).\n",
    "- Flexibility: Code works with any class adhering to the expected interface.\n",
    "### Polymorphism Simplified (With a Real-Life Analogy)\n",
    "- Imagine you have a universal remote control that works with any TV brand (Sony, Samsung, LG).\n",
    "- Same Button (\"Power\") ‚Üí Different TVs respond differently (but all turn ON/OFF).\n",
    "- You don‚Äôt need to know how each TV works internally.\n",
    "- This is polymorphism in action!\n",
    "### Why Important in Data Engineering?\n",
    "- Pipeline Abstraction: Process data from multiple sources (CSV, API, DB) uniformly.\n",
    "- Extensibility: Add new data sources without modifying pipeline logic.\n",
    "- Interchangeability: Swap readers (e.g., CSVReader ‚Üí ParquetReader) seamlessly.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4de9b3ae-41c9-4fee-9f86-a88d643f4fbe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def run_reader(reader):  # Accepts any object with a read() method\n",
    "    print(reader.read())  # Polymorphic call\n",
    "\n",
    "run_reader(CSVReader())  # Output: \"üìÑ Reading from CSV\"\n",
    "run_reader(APIReader())  # Output: \"üåê Fetching from API\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2f859401-e15f-4ad2-8a04-964f9831590e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Abstraction ‚Äì Enforce Structure Across Team\n",
    "## What is Abstraction?\n",
    "- Hides complex details, exposes only what‚Äôs necessary.\n",
    "## Real-Life Example: Driving a Car\n",
    "### üöó What you see (Simplified Interface):\n",
    "- Steering wheel\n",
    "- Accelerator\n",
    "- Brake\n",
    "### üîß What‚Äôs hidden (Complex Internals):\n",
    "- Engine combustion\n",
    "- Gear mechanisms\n",
    "- Fuel injection\n",
    "\n",
    "‚úÖ You don‚Äôt need to know how the engine works to drive!\n",
    "\n",
    "‚Üí This is abstraction in action.\n",
    "- Enforces structure (e.g., \"All ingestors must have a read() method\").\n",
    "Uses Abstract Base Classes (ABCs) to define rules.\n",
    "## Why Important in Data Engineering?\n",
    "- ‚úÖ Team Standardization ‚Äì Ensures everyone implements required methods.\n",
    "- ‚úÖ Prevents Mistakes ‚Äì No incomplete classes (e.g., a DBIngestor without read()).\n",
    "- ‚úÖ Clean Contracts ‚Äì \"If it‚Äôs an ingestor, it must have these methods.\"\n",
    "\n",
    "## Interview Answer (Simple & Powerful)\n",
    "\"Abstraction is like a rulebook for classes. The Ingestor ABC says: ‚ÄòIf you‚Äôre an ingestor, you MUST have a read() method.‚Äô This ensures all ingestors (CSV, API, DB) work the same way, making the code predictable and easy to extend.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a9d9dd63-b288-44cc-9bf4-e7d17cf65c15",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 1. Abstract Base Class (Blueprint)\n",
    "from abc import ABC, abstractmethod\n",
    "\n",
    "class Ingestor(ABC):  # Abstract class (cannot be instantiated)\n",
    "    @abstractmethod\n",
    "    def read(self):  # Must be implemented by child classes\n",
    "        pass\n",
    "# 2. Concrete Implementations\n",
    "class CSVIngestor(Ingestor):\n",
    "    def read(self):  # Must implement read() (or Python raises an error)\n",
    "        return \"Reading CSV...\"\n",
    "\n",
    "class APIIngestor(Ingestor):\n",
    "    def read(self):  # Must implement read()\n",
    "        return \"Calling API...\"\n",
    "# 3. Polymorphic Execution    \n",
    "def run(ingestor):  # Works with ANY Ingestor subclass\n",
    "    print(ingestor.read())\n",
    "\n",
    "run(CSVIngestor())  # Output: \"Reading CSV...\"\n",
    "run(APIIngestor())  # Output: \"Calling API...\"   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a8327e0d-c54d-47da-a6f5-7c9d33bcda3b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Real Data Pipeline: OOP Version\n",
    "- (With Real-Life Analogy & Key Concepts)\n",
    "### The Pipeline Structure (Like a Factory Assembly Line)\n",
    "This pipeline has 3 specialized classes, each doing one job:\n",
    "- Class\tReal-Life Analogy\tResponsibility\tMethod\n",
    "- FileIngestor\tRaw Material Supplier\tFetches raw data\tread()\n",
    "- Cleaner\tQuality Control\tFixes errors, formats\tclean()\n",
    "- Writer\tPackaging & Shipping\tSaves final product\twrite()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "49acfba2-0ab3-4395-8012-e69e660c96cd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# (1) FileIngestor - Gets Raw Data\n",
    "class FileIngestor:\n",
    "    def __init__(self, path):  # Constructor: Needs file path\n",
    "        self.path = path        # Stores path as an attribute\n",
    "\n",
    "    def read(self):            # Fetches data\n",
    "        print(f\"Reading data from {self.path}\")\n",
    "        return f\"Raw data from {self.path}\"  # Simulated output    \n",
    "# Key Points:\n",
    "# Encapsulation: path is stored internally.\n",
    "# Single Responsibility: Only reads files, nothing else.\n",
    "\n",
    "# (2) Cleaner - Processes Data\n",
    "class Cleaner:\n",
    "    def clean(self, data):     # Takes raw data, returns cleaned\n",
    "        print(\"Cleaning data...\")\n",
    "        return f\"Cleaned version of: {data}\"  # Adds metadata\n",
    "# Key Points:\n",
    "# Reusable: Can clean data from any source (CSV, API, etc.).\n",
    "# Separation of Concerns: Doesn‚Äôt know/care where data came from.\n",
    "\n",
    "# (3) Writer - Saves Final Output\n",
    "class Writer:\n",
    "    def write(self, data):     # Takes cleaned data\n",
    "        print(f\"Writing data: {data}\")\n",
    "        return \"Write success\"  # Confirmation\n",
    "# Key Points:\n",
    "# Flexible: Could write to DB, cloud, etc. (extend later).\n",
    "# Loose Coupling: Doesn‚Äôt depend on FileIngestor or Cleaner.\n",
    "\n",
    "# Pipeline Execution\n",
    "# Assemble the pipeline\n",
    "reader = FileIngestor(\"data.csv\")  # Step 1: Configure reader\n",
    "raw_data = reader.read()          # Step 2: Extract\n",
    "\n",
    "cleaned = Cleaner().clean(raw_data)  # Step 3: Transform\n",
    "Writer().write(cleaned)              # Step 4: Load"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "OOPs in python",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
